# Great Papers Towards LLM Intelligence

Researchers have made remarkable and groundbreaking achievements in exploring the mechanisms and the fundamental nature of intelligence in AI models, particularly LLMs. This paper repository aims to document these milestones, providing a quick overview of the advancements in this field.

**Feature Extracting**

Automatically Interpreting Millions of Features in Large Language Models [link](https://arxiv.org/pdf/2410.13928) 2024.10

Using Dictionary Learning Features as Classifiers [link](https://transformer-circuits.pub/2024/features-as-classifiers/index.html) 2024.10

Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models [link](https://arxiv.org/pdf/2410.06981) 2024.10

Interpretability Evals for Dictionary Learning [link](https://transformer-circuits.pub/2024/august-update/index.html#interp-evals) 2024.08

Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2 [link](https://arxiv.org/pdf/2408.05147) 2024.08

Scaling and evaluating sparse autoencoders [link](https://cdn.openai.com/papers/sparse-autoencoders.pdf) 2024.06

Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet [link](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html) 2024.05

Reflections on Qualitative Research [link](https://transformer-circuits.pub/2024/qualitative-essay/index.html) 2024.03

Towards Monosemanticity: Decomposing Language Models With Dictionary Learning [link](https://transformer-circuits.pub/2023/monosemantic-features/index.html) 2023.10

Sparse Autoencoders Find Highly Interpretable Features in Language Models [link](https://arxiv.org/pdf/2309.08600) 2023.09

Toy Models of Superposition [link](https://transformer-circuits.pub/2022/toy_model/index.html) 2022.09

**Neuron & Circuits Analysis**

Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis [link](https://arxiv.org/pdf/2409.14144) 2024.09

Hypothesis Testing the Circuit Hypothesis in LLMs [link](https://openreview.net/forum?id=ibSNv9cldu) 2024.06

RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations [link](https://arxiv.org/pdf/2402.17700) 2024.02

Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models [link](https://arxiv.org/pdf/2402.16438) 2024.02

Successor Heads: Recurring, Interpretable Attention Heads In The Wild [link](https://arxiv.org/abs/2312.09230) 2023.12

**Human Comparisons (Cognition & linguistics)**

The Platonic Representation Hypothesis [link](https://arxiv.org/pdf/2405.07987) 2024.07

Shared functional specialization in transformer-based language models and the human brain [link](https://www.nature.com/articles/s41467-024-49173-5) 2024.06

Alignment of brain embeddings and artificial contextual embeddings in natural language points to common geometric patterns [link](https://pubmed.ncbi.nlm.nih.gov/38553456/) 2024.05

Semantic Structure in Deep Learning [link](https://www.annualreviews.org/docserver/fulltext/linguistics/8/1/annurev-linguistics-031120-122924.pdf?expires=1729404555&id=id&accname=guest&checksum=BAC354EE7D776B2E3F70062009260611) 2022.01
